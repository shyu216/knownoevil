---
title: COMP90042 Natural Language Processing
shortTitle: COMP90042
order: 1
icon: book-open
category:
  - UniMelb
  - 24S1
tag:
  - Artificial Intelligence
  - Machine Learning
  - Deep Learning
  - Natural Language Processing
---

## 1. Preprocessing
- Sentence segmentation 
- Tokenization, subword tokenization
- Word normalization
  - Inflectional vs derivational morphology
  - Lemmatization vs stemming
- Stopword removal

## 2. N-gram Language Model
- Derivation
- Smoothing techniques
  - Add-k
  - Absolute discounting
  - Katz backoff
  - Kneser-Ney smoothing
  - Interpolation

## 3. Text Classification
- Build a classifier
- Task
  - Topic classification
  - Sentiment analysis
  - Native language identification
- Algorithms
  - Naive Bayes, logistic regression, SVM
  - kNN, neural networks
- Bias vs variance：欠拟合under和过拟合over的取舍
- Evaluation
  - Precision, recall, F1

## 4. Part of Speech Tagging
- English POS
  - Closed vs open classes
- Tagsets
  - Penn Treebank tagset
- Automatic taggers
  - Rule-based
  - Statistical
    - Unigram, classifier-based, HMM

## 5. Hidden Markov Model
- Probabilistic formulation:
  - Emission & Transition
- Training
- Viterbi algorithm
- Generative vs discriminative models

## 6. Feedforward Neural Network
- Formulation
- Tasks:
  - Topic classifcation
  - Language models
  - POS tagging
- Word embeddings
- Convolutional networks

## 7. Recurrent Neural Network
- Formulation
- RNN: language models
- LSTM:
  - Functions of gates
  - Variants
- Tasks:
  - Text classification: sentiment analysis
  - POS tagging

## 8. Lexical Semantics
- Definition of word sense, gloss
- Lexical Relationship:
  - Synonymy, antonymy, hypernymy, meronymy
- Structure of wordnet
- Word similarity
  - Path length
  - Depth information
  - Information content
- Word sense unambiguation
  - supervised, unsupervised

## 9. Distributional Semantics
- Matrices:
  - VSM, TF-IDF, word-word co-occurrence
- Association measures: PMI, PPMI
- Count-based method: SVM
- Neural method: skip-gram, CBOW
- Evaluation:
  - Word similarity, analogy

## 10. Contextual Representation
- Formulation with RNN
- ELMo
- BERT
  - Objective
  - Fine-tuning for downstream tasks
- Transformers
  - Multi-head attention

## 11. Attention
- Sequential model with attention
- Attention variants:
  - Concat
  - Dot product
  - Scaled dot product
  - Location-based
  - Cosine similarity
- Global vs local attention
- Self-attention

## 12. Machine Translation
- Statistical MT
- Neural MT with teacher forcing

## 13. Transformer
- Mutli-head self-attention
- Position encoding

## 14. Pretrained Language Models
- Encoder architecture
  - BERT
  - bidirectional context
  - good for classification
- Encoder-decoder architecture
- Decoder architecture
  - GPT
  - unidirectional context
  - good for generation

## 15. Large Language Models
- In-context learning
- Step-by-step reasoning
- Human-feedback reinforcement
- Instruction following

## 16. Named Entity Recognition 
- Predict entities in a text
- Traditional ML methods
- Bi-LSTM with another RNN/CRF
- Multi-aspect NER

## 17. Coreference Resolution 共指消解
- Coreference, anaphora
- B-Cubed metric

## 18. Question Answering and Reading Comprehension
- Knowledge-based QA
- Visual QA

## 19. Spoken Language Understanding
- Attention RNNs
- Joint BERT
- Tri-level model
- Explainable NLU

## 20. Vision Language Pretrained Model
- V-L Interaction Model
  - Self-attention, co-attention, VSE
- Constrastive Language-Image Pretraining 

## 21. Ethics
- Sotial bias 刻板印象
- Incivility 仇恨言论
- Privacy Violation 歧视
- Misinformation 谣言
- Technological divide 发展不平衡