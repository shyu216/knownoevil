import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,o as e,f as o}from"./app-Dit9sr0-.js";const t={},n=o('<h1 id="" tabindex="-1"><a class="header-anchor" href="#"><span></span></a></h1><h1 id="natural-language-processing" tabindex="-1"><a class="header-anchor" href="#natural-language-processing"><span>Natural Language Processing</span></a></h1><p>First half:</p><ol><li>Preprocessing <ol><li>Sentence segmentation</li><li>Tokenization, subword tokenization</li><li>Word normalization <ol><li>Inflectional vs derivational morphology</li><li>Lemmatization vs stemming</li></ol></li><li>Stopword removal</li></ol></li><li>N-gram Language Model <ol><li>Derivation</li><li>Smoothing techniques <ol><li>Add-k</li><li>Absolute discounting</li><li>Katz backoff</li><li>Kneser-Ney smoothing</li><li>Interpolation</li></ol></li></ol></li><li>Text Classification <ol><li>Build a classifier</li><li>Task <ol><li>Topic classification</li><li>Sentiment analysis</li><li>Native language identification</li></ol></li><li>Algorithms <ol><li>Naive Bayes, logistic regression, SVM</li><li>kNN, neural networks</li></ol></li><li>Bias vs variance：欠拟合under和过拟合over的取舍</li><li>Evaluation <ol><li>Precision, recall, F1</li></ol></li></ol></li><li>Part of Speech Tagging <ol><li>English POS <ol><li>Closed vs open classes</li></ol></li><li>Tagsets <ol><li>Penn Treebank tagset</li></ol></li><li>Automatic taggers <ol><li>Rule-based</li><li>Statistical <ol><li>Unigram, classifier-based, HMM</li></ol></li></ol></li></ol></li><li>Hidden Markov Model <ol><li>Probabilistic formulation: <ol><li>Emission &amp; Transition</li></ol></li><li>Training</li><li>Viterbi algorithm</li><li>Generative vs discriminative models</li></ol></li><li>Feedforward Neural Network <ol><li>Formulation</li><li>Tasks: <ol><li>Topic classifcation</li><li>Language models</li><li>POS tagging</li></ol></li><li>Word embeddings</li><li>Convolutional networks</li></ol></li><li>Recurrent Neural Network <ol><li>Formulation</li><li>RNN: language models</li><li>LSTM: <ol><li>Functions of gates</li><li>Variants</li></ol></li><li>Tasks: <ol><li>Text classification: sentiment analysis</li><li>POS tagging</li></ol></li></ol></li><li>Lexical Semantics <ol><li>Definition of word sense, gloss</li><li>Lexical Relationship: <ol><li>Synonymy, antonymy, hypernymy, meronymy</li></ol></li><li>Structure of wordnet</li><li>Word similarity <ol><li>Path lenght</li><li>Depth information</li><li>Information content</li></ol></li><li>Word sense unambiguation <ol><li>supervised, unsupervised</li></ol></li></ol></li><li>Distributional Semantics <ol><li>Matrices: <ol><li>VSM, TF-IDF, word-word co-occurrence</li></ol></li><li>Association measures: PMI, PPMI</li><li>Count-based method: SVM</li><li>Neural method: skip-gram, CBOW</li><li>Evaluation: <ol><li>Word similarity, analogy</li></ol></li></ol></li><li>Contextual Representation <ol><li>Formulation with RNN</li><li>ELMo</li><li>BERT <ol><li>Objective</li><li>Fine-tuning for downstream tasks</li></ol></li><li>Transformers <ol><li>Multi-head attention</li></ol></li></ol></li></ol><p>Second half:</p><ol><li>Attention <ol><li>Sequential model with attention</li><li>Attention variants: <ol><li>Concat</li><li>Dot product</li><li>Scaled dot product</li><li>Location-based</li><li>Cosine similarity</li></ol></li><li>Global vs local attention</li><li>Self-attention</li></ol></li><li>Machine Translation <ol><li>Statistical MT</li><li>Neural MT with teacher forcing</li></ol></li><li>Transformer <ol><li>Mutli-head self-attention</li><li>Position encoding</li></ol></li><li>Pretrained Language Models <ol><li>Encoder architecture <ol><li>BERT</li><li>bidirectional context</li><li>good for classification</li></ol></li><li>Encoder-decoder architecture</li><li>Decoder architecture <ol><li>GPT</li><li>unidirectional context</li><li>good for generation</li></ol></li></ol></li><li>Large Language Models <ol><li>In-context learning</li><li>Step-by-step reasoning</li><li>Human-feedback reinforcement</li><li>Instruction following</li></ol></li><li>Named Entity Recognition <ol><li>Predict entities in a text</li><li>Traditional ML methods</li><li>Bi-LSTM with another RNN/CRF</li><li>Multi-aspect NER</li></ol></li><li>Coreference Resolution 共指消解 <ol><li>Coreference, anaphora</li><li>B-Cubed metric</li></ol></li><li>Question Answering and Reading Comprehension <ol><li>Knowledge-based QA</li><li>Visual QA</li></ol></li><li>Spoken Language Understanding <ol><li>Attention RNNs</li><li>Joint BERT</li><li>Tri-level model</li><li>Explainable NLU</li></ol></li><li>Vision Language Pretrained Model <ol><li>V-L Interaction Model <ol><li>Self-attention, co-attention, VSE</li></ol></li><li>Constrastive Language-Image Pretraining</li></ol></li><li>Ethics <ol><li>Sotial bias 刻板印象</li><li>Incivility 仇恨言论</li><li>Privacy Violation 歧视</li><li>Misinformation 谣言</li><li>Technological divide 发展不平衡</li></ol></li></ol>',6),a=[n];function r(s,c){return e(),l("div",null,a)}const m=i(t,[["render",r],["__file","COMP90042.html.vue"]]),u=JSON.parse('{"path":"/master/COMP90042.html","title":"COMP90042","lang":"en-US","frontmatter":{"title":"COMP90042","icon":"book-open","category":["UniMelb","24S1"],"tag":["Artificial Intelligence","Machine Learning","Deep Learning","Natural Language Processing"],"description":"Natural Language Processing First half: Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmati...","head":[["meta",{"property":"og:url","content":"https://shyu216.github.io/knownoevil/knownoevil/master/COMP90042.html"}],["meta",{"property":"og:site_name","content":"Know No Evil"}],["meta",{"property":"og:title","content":"COMP90042"}],["meta",{"property":"og:description","content":"Natural Language Processing First half: Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmati..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-08-01T05:48:15.000Z"}],["meta",{"property":"article:author","content":"Dale"}],["meta",{"property":"article:tag","content":"Artificial Intelligence"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:tag","content":"Deep Learning"}],["meta",{"property":"article:tag","content":"Natural Language Processing"}],["meta",{"property":"article:modified_time","content":"2024-08-01T05:48:15.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"COMP90042\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-08-01T05:48:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Dale\\",\\"url\\":\\"https://github.com/shyu216\\"}]}"]]},"headers":[],"git":{"createdTime":1722491295000,"updatedTime":1722491295000,"contributors":[{"name":"shyu216","email":"sihong1@student.unimelb.edu.au","commits":1}]},"readingTime":{"minutes":1.86,"words":558},"filePathRelative":"master/COMP90042.md","localizedDate":"August 1, 2024","autoDesc":true}');export{m as comp,u as data};
