import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,e as n,o as i}from"./app-CQHzVapx.js";const l={};function s(a,e){return i(),o("div",null,e[0]||(e[0]=[n('<ul><li><a href="https://docs.ultralytics.com" target="_blank" rel="noopener noreferrer">YOLO Documentation</a></li></ul><hr><h2 id="what-does-the-instance-segmentation-model-look-like" tabindex="-1"><a class="header-anchor" href="#what-does-the-instance-segmentation-model-look-like"><span>What does the instance segmentation model look like?</span></a></h2><ul><li><p><strong>Input size:</strong> Usually 640 × 640.</p></li><li><p><strong>Detection output shape:</strong><br><code>(1, 116, 8400)</code></p><ul><li>Batch size: 1</li><li>116 = 4 (x, y, w, h) + 80 (class confidences) + 32 (mask weights)</li><li>8400: Number of anchor points (depends on model configuration)</li></ul></li><li><p><strong>Mask output shape:</strong><br><code>(1, 32, 160, 160)</code></p><ul><li>32 masks, each of size 160 × 160</li><li>Each mask represents an object or a mask prototype in the image</li></ul></li><li><p><strong>Post-processing:</strong></p><ul><li>Masks are upsampled to the input image size using <strong>bilinear interpolation</strong></li><li>A <strong>sigmoid activation</strong> is applied to produce the final mask values in the range [0, 1]</li></ul></li></ul><hr><h2 id="what-does-the-pose-estimation-model-look-like" tabindex="-1"><a class="header-anchor" href="#what-does-the-pose-estimation-model-look-like"><span>What does the pose estimation model look like?</span></a></h2><ul><li><p><strong>Keypoint names:</strong><br><code>[&#39;Nose&#39;, &#39;Left Eye&#39;, &#39;Right Eye&#39;, &#39;Left Ear&#39;, &#39;Right Ear&#39;, &#39;Left Shoulder&#39;, &#39;Right Shoulder&#39;, &#39;Left Elbow&#39;, &#39;Right Elbow&#39;, &#39;Left Wrist&#39;, &#39;Right Wrist&#39;, &#39;Left Hip&#39;, &#39;Right Hip&#39;, &#39;Left Knee&#39;, &#39;Right Knee&#39;, &#39;Left Ankle&#39;, &#39;Right Ankle&#39;]</code></p></li><li><p><strong>Output shape:</strong><br><code>(1, 56, 8400)</code></p><ul><li>56 = 4 (x, y, w, h) + 1 (confidence) + 17 × 3 (keypoints: x, y, confidence)</li><li>8400: Number of anchor points</li></ul></li></ul>',7)]))}const c=t(l,[["render",s]]),h=JSON.parse(`{"path":"/dev/yolo.html","title":"You Only Look Once (YOLO)","lang":"en-US","frontmatter":{"title":"You Only Look Once (YOLO)","shortTitle":"YOLO","icon":"mug-hot","description":"YOLO Documentation What does the instance segmentation model look like? Input size: Usually 640 × 640. Detection output shape: (1, 116, 8400) Batch size: 1 116 = 4 (x, y, w, h) ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"You Only Look Once (YOLO)\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-07-09T23:58:34.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Dale\\",\\"url\\":\\"https://github.com/shyu216\\"}]}"],["meta",{"property":"og:url","content":"https://shyu216.github.io/knownoevil/knownoevil/dev/yolo.html"}],["meta",{"property":"og:site_name","content":"SIHONG's Blog"}],["meta",{"property":"og:title","content":"You Only Look Once (YOLO)"}],["meta",{"property":"og:description","content":"YOLO Documentation What does the instance segmentation model look like? Input size: Usually 640 × 640. Detection output shape: (1, 116, 8400) Batch size: 1 116 = 4 (x, y, w, h) ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-07-09T23:58:34.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-09T23:58:34.000Z"}]]},"git":{"createdTime":1751855515000,"updatedTime":1752105514000,"contributors":[{"name":"shyu216","username":"shyu216","email":"yusihong073@gmail.com","commits":2,"url":"https://github.com/shyu216"}]},"filePathRelative":"dev/yolo.md","excerpt":"<ul>\\n<li><a href=\\"https://docs.ultralytics.com\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">YOLO Documentation</a></li>\\n</ul>\\n<hr>\\n<h2>What does the instance segmentation model look like?</h2>\\n<ul>\\n<li>\\n<p><strong>Input size:</strong> Usually 640 × 640.</p>\\n</li>\\n<li>\\n<p><strong>Detection output shape:</strong><br>\\n<code>(1, 116, 8400)</code></p>\\n<ul>\\n<li>Batch size: 1</li>\\n<li>116 = 4 (x, y, w, h) + 80 (class confidences) + 32 (mask weights)</li>\\n<li>8400: Number of anchor points (depends on model configuration)</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>Mask output shape:</strong><br>\\n<code>(1, 32, 160, 160)</code></p>\\n<ul>\\n<li>32 masks, each of size 160 × 160</li>\\n<li>Each mask represents an object or a mask prototype in the image</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>Post-processing:</strong></p>\\n<ul>\\n<li>Masks are upsampled to the input image size using <strong>bilinear interpolation</strong></li>\\n<li>A <strong>sigmoid activation</strong> is applied to produce the final mask values in the range [0, 1]</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}`);export{c as comp,h as data};
