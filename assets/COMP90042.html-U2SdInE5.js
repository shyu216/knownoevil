import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,o as l,a as n}from"./app-BlBqP3ms.js";const a={},t=n('<h2 id="_1-preprocessing" tabindex="-1"><a class="header-anchor" href="#_1-preprocessing"><span>1. Preprocessing</span></a></h2><ul><li>sentence segmentation</li><li>tokenization, subword tokenization</li><li>word normalization <ul><li>inflectional vs derivational morphology</li><li>lemmatization vs stemming</li></ul></li><li>stopword removal</li></ul><h2 id="_2-n-gram-language-model" tabindex="-1"><a class="header-anchor" href="#_2-n-gram-language-model"><span>2. N-gram Language Model</span></a></h2><ul><li>derivation</li><li>smoothing techniques <ul><li>add-k</li><li>absolute discounting</li><li>Katz backoff</li><li>Kneser-Ney smoothing</li><li>interpolation</li></ul></li></ul><h2 id="_3-text-classification" tabindex="-1"><a class="header-anchor" href="#_3-text-classification"><span>3. Text Classification</span></a></h2><ul><li>build a classifier</li><li>task <ul><li>topic classification</li><li>sentiment analysis</li><li>native language identification</li></ul></li><li>algorithms <ul><li>naive bayes, logistic regression, SVM</li><li>kNN, neural networks</li></ul></li><li>bias vs variance：欠拟合under和过拟合over的取舍</li><li>evaluation <ul><li>precision, recall, F1</li></ul></li></ul><h2 id="_4-part-of-speech-tagging" tabindex="-1"><a class="header-anchor" href="#_4-part-of-speech-tagging"><span>4. Part of Speech Tagging</span></a></h2><ul><li>english POS <ul><li>closed vs open classes</li></ul></li><li>tagsets <ul><li>penn treebank tagset</li></ul></li><li>automatic taggers <ul><li>rule-based</li><li>statistical <ul><li>unigram, classifier-based, HMM</li></ul></li></ul></li></ul><h2 id="_5-hidden-markov-model" tabindex="-1"><a class="header-anchor" href="#_5-hidden-markov-model"><span>5. Hidden Markov Model</span></a></h2><ul><li>probabilistic formulation: <ul><li>emission &amp; transition</li></ul></li><li>training</li><li>Viterbi algorithm</li><li>generative vs discriminative models</li></ul><h2 id="_6-feedforward-neural-network" tabindex="-1"><a class="header-anchor" href="#_6-feedforward-neural-network"><span>6. Feedforward Neural Network</span></a></h2><ul><li>formulation</li><li>tasks: <ul><li>topic classification</li><li>language models</li><li>POS tagging</li></ul></li><li>word embeddings</li><li>convolutional networks</li></ul><h2 id="_7-recurrent-neural-network" tabindex="-1"><a class="header-anchor" href="#_7-recurrent-neural-network"><span>7. Recurrent Neural Network</span></a></h2><ul><li>formulation</li><li>RNN: language models</li><li>LSTM: <ul><li>functions of gates</li><li>variants</li></ul></li><li>tasks: <ul><li>text classification: sentiment analysis</li><li>POS tagging</li></ul></li></ul><h2 id="_8-lexical-semantics" tabindex="-1"><a class="header-anchor" href="#_8-lexical-semantics"><span>8. Lexical Semantics</span></a></h2><ul><li>definition of word sense, gloss</li><li>lexical relationship: <ul><li>synonymy, antonymy, hypernymy, meronymy</li></ul></li><li>structure of wordnet</li><li>word similarity <ul><li>path length</li><li>depth information</li><li>information content</li></ul></li><li>word sense unambiguation <ul><li>supervised, unsupervised</li></ul></li></ul><h2 id="_9-distributional-semantics" tabindex="-1"><a class="header-anchor" href="#_9-distributional-semantics"><span>9. Distributional Semantics</span></a></h2><ul><li>matrices: <ul><li>VSM, TF-IDF, word-word co-occurrence</li></ul></li><li>association measures: PMI, PPMI</li><li>count-based method: SVM</li><li>neural method: skip-gram, CBOW</li><li>evaluation: <ul><li>word similarity, analogy</li></ul></li></ul><h2 id="_10-contextual-representation" tabindex="-1"><a class="header-anchor" href="#_10-contextual-representation"><span>10. Contextual Representation</span></a></h2><ul><li>formulation with RNN</li><li>ELMo</li><li>BERT <ul><li>objective</li><li>fine-tuning for downstream tasks</li></ul></li><li>transformers <ul><li>multi-head attention</li></ul></li></ul><h2 id="_11-attention" tabindex="-1"><a class="header-anchor" href="#_11-attention"><span>11. Attention</span></a></h2><ul><li>sequential model with attention</li><li>attention variants: <ul><li>concat</li><li>dot product</li><li>scaled dot product</li><li>location-based</li><li>cosine similarity</li></ul></li><li>global vs local attention</li><li>self-attention</li></ul><h2 id="_12-machine-translation" tabindex="-1"><a class="header-anchor" href="#_12-machine-translation"><span>12. Machine Translation</span></a></h2><ul><li>statistical MT</li><li>neural MT with teacher forcing</li></ul><h2 id="_13-transformer" tabindex="-1"><a class="header-anchor" href="#_13-transformer"><span>13. Transformer</span></a></h2><ul><li>multi-head self-attention</li><li>position encoding</li></ul><h2 id="_14-pretrained-language-models" tabindex="-1"><a class="header-anchor" href="#_14-pretrained-language-models"><span>14. Pretrained Language Models</span></a></h2><ul><li>encoder architecture <ul><li>BERT</li><li>bidirectional context</li><li>good for classification</li></ul></li><li>encoder-decoder architecture</li><li>decoder architecture <ul><li>GPT</li><li>unidirectional context</li><li>good for generation</li></ul></li></ul><h2 id="_15-large-language-models" tabindex="-1"><a class="header-anchor" href="#_15-large-language-models"><span>15. Large Language Models</span></a></h2><ul><li>in-context learning</li><li>step-by-step reasoning</li><li>human-feedback reinforcement</li><li>instruction following</li></ul><h2 id="_16-named-entity-recognition" tabindex="-1"><a class="header-anchor" href="#_16-named-entity-recognition"><span>16. Named Entity Recognition</span></a></h2><ul><li>predict entities in a text</li><li>traditional ML methods</li><li>bi-LSTM with another RNN/CRF</li><li>multi-aspect NER</li></ul><h2 id="_17-coreference-resolution-共指消解" tabindex="-1"><a class="header-anchor" href="#_17-coreference-resolution-共指消解"><span>17. Coreference Resolution 共指消解</span></a></h2><ul><li>coreference, anaphora</li><li>B-cubed metric</li></ul><h2 id="_18-question-answering-and-reading-comprehension" tabindex="-1"><a class="header-anchor" href="#_18-question-answering-and-reading-comprehension"><span>18. Question Answering and Reading Comprehension</span></a></h2><ul><li>knowledge-based QA</li><li>visual QA</li></ul><h2 id="_19-spoken-language-understanding" tabindex="-1"><a class="header-anchor" href="#_19-spoken-language-understanding"><span>19. Spoken Language Understanding</span></a></h2><ul><li>attention RNNs</li><li>joint BERT</li><li>tri-level model</li><li>explainable NLU</li></ul><h2 id="_20-vision-language-pretrained-model" tabindex="-1"><a class="header-anchor" href="#_20-vision-language-pretrained-model"><span>20. Vision Language Pretrained Model</span></a></h2><ul><li>V-L interaction model <ul><li>self-attention, co-attention, VSE</li></ul></li><li>constrastive language-image pretraining</li></ul><h2 id="_21-ethics" tabindex="-1"><a class="header-anchor" href="#_21-ethics"><span>21. Ethics</span></a></h2><ul><li>social bias 刻板印象</li><li>incivility 仇恨言论</li><li>privacy violation 歧视</li><li>misinformation 谣言</li><li>technological divide 发展不平衡</li></ul>',42),o=[t];function r(s,c){return l(),i("div",null,o)}const g=e(a,[["render",r],["__file","COMP90042.html.vue"]]),h=JSON.parse('{"path":"/master/COMP90042.html","title":"COMP90042 Natural Language Processing","lang":"en-US","frontmatter":{"title":"COMP90042 Natural Language Processing","shortTitle":"COMP90042","order":1,"icon":"book-open","category":["UniMelb","24S1"],"tag":["Artificial Intelligence","Machine Learning","Deep Learning","Natural Language Processing"],"description":"1. Preprocessing sentence segmentation tokenization, subword tokenization word normalization inflectional vs derivational morphology lemmatization vs stemming stopword removal 2...","head":[["meta",{"property":"og:url","content":"https://shyu216.github.io/knownoevil/knownoevil/master/COMP90042.html"}],["meta",{"property":"og:site_name","content":"Know No Evil"}],["meta",{"property":"og:title","content":"COMP90042 Natural Language Processing"}],["meta",{"property":"og:description","content":"1. Preprocessing sentence segmentation tokenization, subword tokenization word normalization inflectional vs derivational morphology lemmatization vs stemming stopword removal 2..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-08-26T12:56:08.000Z"}],["meta",{"property":"article:author","content":"Dale"}],["meta",{"property":"article:tag","content":"Artificial Intelligence"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:tag","content":"Deep Learning"}],["meta",{"property":"article:tag","content":"Natural Language Processing"}],["meta",{"property":"article:modified_time","content":"2024-08-26T12:56:08.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"COMP90042 Natural Language Processing\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-08-26T12:56:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Dale\\",\\"url\\":\\"https://github.com/shyu216\\"}]}"]]},"headers":[{"level":2,"title":"1. Preprocessing","slug":"_1-preprocessing","link":"#_1-preprocessing","children":[]},{"level":2,"title":"2. N-gram Language Model","slug":"_2-n-gram-language-model","link":"#_2-n-gram-language-model","children":[]},{"level":2,"title":"3. Text Classification","slug":"_3-text-classification","link":"#_3-text-classification","children":[]},{"level":2,"title":"4. Part of Speech Tagging","slug":"_4-part-of-speech-tagging","link":"#_4-part-of-speech-tagging","children":[]},{"level":2,"title":"5. Hidden Markov Model","slug":"_5-hidden-markov-model","link":"#_5-hidden-markov-model","children":[]},{"level":2,"title":"6. Feedforward Neural Network","slug":"_6-feedforward-neural-network","link":"#_6-feedforward-neural-network","children":[]},{"level":2,"title":"7. Recurrent Neural Network","slug":"_7-recurrent-neural-network","link":"#_7-recurrent-neural-network","children":[]},{"level":2,"title":"8. Lexical Semantics","slug":"_8-lexical-semantics","link":"#_8-lexical-semantics","children":[]},{"level":2,"title":"9. Distributional Semantics","slug":"_9-distributional-semantics","link":"#_9-distributional-semantics","children":[]},{"level":2,"title":"10. Contextual Representation","slug":"_10-contextual-representation","link":"#_10-contextual-representation","children":[]},{"level":2,"title":"11. Attention","slug":"_11-attention","link":"#_11-attention","children":[]},{"level":2,"title":"12. Machine Translation","slug":"_12-machine-translation","link":"#_12-machine-translation","children":[]},{"level":2,"title":"13. Transformer","slug":"_13-transformer","link":"#_13-transformer","children":[]},{"level":2,"title":"14. Pretrained Language Models","slug":"_14-pretrained-language-models","link":"#_14-pretrained-language-models","children":[]},{"level":2,"title":"15. Large Language Models","slug":"_15-large-language-models","link":"#_15-large-language-models","children":[]},{"level":2,"title":"16. Named Entity Recognition","slug":"_16-named-entity-recognition","link":"#_16-named-entity-recognition","children":[]},{"level":2,"title":"17. Coreference Resolution 共指消解","slug":"_17-coreference-resolution-共指消解","link":"#_17-coreference-resolution-共指消解","children":[]},{"level":2,"title":"18. Question Answering and Reading Comprehension","slug":"_18-question-answering-and-reading-comprehension","link":"#_18-question-answering-and-reading-comprehension","children":[]},{"level":2,"title":"19. Spoken Language Understanding","slug":"_19-spoken-language-understanding","link":"#_19-spoken-language-understanding","children":[]},{"level":2,"title":"20. Vision Language Pretrained Model","slug":"_20-vision-language-pretrained-model","link":"#_20-vision-language-pretrained-model","children":[]},{"level":2,"title":"21. Ethics","slug":"_21-ethics","link":"#_21-ethics","children":[]}],"git":{"createdTime":1722491295000,"updatedTime":1724676968000,"contributors":[{"name":"shyu216","email":"sihong1@student.unimelb.edu.au","commits":3},{"name":"shyu216","email":"yusihong073@gmail.com","commits":2}]},"readingTime":{"minutes":1.45,"words":436},"filePathRelative":"master/COMP90042.md","localizedDate":"August 1, 2024","autoDesc":true,"excerpt":"<h2>1. Preprocessing</h2>\\n<ul>\\n<li>sentence segmentation</li>\\n<li>tokenization, subword tokenization</li>\\n<li>word normalization\\n<ul>\\n<li>inflectional vs derivational morphology</li>\\n<li>lemmatization vs stemming</li>\\n</ul>\\n</li>\\n<li>stopword removal</li>\\n</ul>\\n<h2>2. N-gram Language Model</h2>\\n<ul>\\n<li>derivation</li>\\n<li>smoothing techniques\\n<ul>\\n<li>add-k</li>\\n<li>absolute discounting</li>\\n<li>Katz backoff</li>\\n<li>Kneser-Ney smoothing</li>\\n<li>interpolation</li>\\n</ul>\\n</li>\\n</ul>"}');export{g as comp,h as data};
